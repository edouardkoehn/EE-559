{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformating of the data from MMHS150K dataset\n",
    "\n",
    "This notebook is a part of the project to reformat the data from the MMHS150K dataset. The original dataset is in the form of a JSON file. The data is reformatted into a CSV file for easier access and manipulation. The dataset contains the following columns:\n",
    "- `index`: The unique identifier for each tweet.\n",
    "- `img_url`: The URL of the image associated with the tweet.\n",
    "- `labels`: The labels assigned to the tweet.\n",
    "- `tweet_url`: The URL of the tweet.\n",
    "- `tweet_text`: The text content of the tweet.\n",
    "- `labels_str`: The labels assigned to the tweet as a string.\n",
    "- `tweet_text_clean`: The cleaned version of the tweet text. (no URLS and mentions)\n",
    "- `img_text`: The text extracted from the image, if available. (otherwise NaN)\n",
    "- `text_in_image`: Indicates whether the image contains text or not.\n",
    "- `hate_speech`: The level of hate speech in the tweet. (from 0 to 1)\n",
    "- `binary_hate`: A binary label indicating whether the tweet contains hate speech or not. (threshold at 0.5)\n",
    "- `split`: The split of the dataset (train, test, or val).\n",
    "\n",
    "The notebook expects to find such directory structure:\n",
    "```\n",
    ".\n",
    "├── MMHS150K\n",
    "│   ├── MMHS150K_GT.json\n",
    "│   └── img_resized\n",
    "│   │   ├── 1114679353714016256.jpg\n",
    "│   │   ├── ...\n",
    "│   │   └── 1110368198786846720.jpg\n",
    "│   └── img_txt\n",
    "│   │   ├── 1114679353714016256.json\n",
    "│   │   ├── ...\n",
    "│   │   └── 1110368198786846720.json\n",
    "│   └── splits\n",
    "│   │   ├── train_ids.txt\n",
    "│   │   └── test_ids.txt\n",
    "│   └────── val_ids.txt\n",
    "└── reformat_data.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.getcwd()) + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Will load the text-side of the dataset, put it in a pandas dataframe and save it in a csv file\n",
    "# for ease of use in the future\n",
    "\n",
    "# Data folder\n",
    "DATA_FOLDER = ROOT_DIR+'/data/MMHS150K/MMHS150K_GT.json'\n",
    "# Folder with the img_txt\n",
    "IMG_TEXT_FOLDER = ROOT_DIR+'data/MMHS150K/img_txt/'\n",
    "# Splits folder\n",
    "SPLITS_FOLDER = ROOT_DIR+'data/MMHS150K/splits/'\n",
    "\n",
    "## Load data\n",
    "data = pd.read_json(DATA_FOLDER, orient='index', convert_dates=False, convert_axes=False)\n",
    "data = data.reset_index(drop=False)\n",
    "data['index'] = data['index'].astype('int64')\n",
    "\n",
    "\n",
    "## Clean the tweet text\n",
    "# Keep only the text before https://t.co/\n",
    "data['tweet_text_clean'] = data['tweet_text'].str.split('https://t.co/').str[0]\n",
    "# Replace any occurence of @user with <tag>\n",
    "regex_tag = r'(^|[^@\\w])@(\\w{1,15})\\b'\n",
    "data['tweet_text_clean'] = data['tweet_text_clean'].apply(lambda x: re.sub(regex_tag, '<tag>', x))\n",
    "# Replace nan and '' with the string <empty>\n",
    "data[data['tweet_text_clean'].isna()]['tweet_text_clean'] = '<empty>'\n",
    "data['tweet_text_clean'] = data['tweet_text_clean'].apply(lambda x: '<empty>' if x == '' else x)\n",
    "\n",
    "## Add the text of the image if it exists\n",
    "# Number of files in the folder\n",
    "n_files = len(os.listdir(IMG_TEXT_FOLDER))\n",
    "# Names of the files\n",
    "files = os.listdir(IMG_TEXT_FOLDER)\n",
    "# Add new column in the dataset for the image text, filled with None\n",
    "data['img_text'] = [None]*len(data)\n",
    "# Load each file and add the text to the dataset to the corresponding index\n",
    "for file in files:\n",
    "    index = int(file.split('.')[0])\n",
    "    \n",
    "    # Open the file (json)\n",
    "    with open(IMG_TEXT_FOLDER + file) as f:\n",
    "        file_data = json.load(f)\n",
    "                \n",
    "        # Add the text to the dataset\n",
    "        data.loc[data['index'] == index, 'img_text'] = file_data[\"img_text\"]\n",
    "data['text_in_image'] = data['img_text'].isna().apply(lambda x: not x)\n",
    "\n",
    "## Add the hate_speech label\n",
    "# replace the labels with a single label hateful or not\n",
    "data['hate_speech'] = data.apply(lambda x: np.mean([0 if i == 0 else 1 for i in x['labels']]), axis=1)\n",
    "data['binary_hate'] = data['hate_speech'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "\n",
    "\n",
    "## Add the split\n",
    "# Load the splitsb\n",
    "train = pd.read_csv(SPLITS_FOLDER + 'train_ids.txt', header=None)\n",
    "test = pd.read_csv(SPLITS_FOLDER + 'test_ids.txt', header=None)\n",
    "val = pd.read_csv(SPLITS_FOLDER + 'val_ids.txt', header=None)\n",
    "\n",
    "# Add the split to the dataset if the index is in the split\n",
    "data['split'] = 'train'\n",
    "data.loc[data['index'].isin(test[0]), 'split'] = 'test'\n",
    "data.loc[data['index'].isin(val[0]), 'split'] = 'val'\n",
    "\n",
    "display(data)\n",
    "\n",
    "\n",
    "\n",
    "## Save the dataset\n",
    "data.to_csv(ROOT_DIR+'/data/MMHS150K/MMHS150K.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second CSV is created, containing only the memes for which there is a text in the image. This dataset contains the same columns as the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second version of the dataset with only tweets which have a text in the image\n",
    "data2 = data[data['text_in_image']]\n",
    "\n",
    "# Remove text_in_image column\n",
    "data2 = data2.drop(columns=['text_in_image'])\n",
    "\n",
    "display(data2)\n",
    "\n",
    "# Save the dataset\n",
    "data2.to_csv(os.path.join(ROOT_DIR, \"data\", \"MMHS150K\", \"MMHS150K_with_img_text.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the 372th line of data2\n",
    "display(data2.iloc[372])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
