{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import os\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "import sys\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from src.dataset import CustomDataset\n",
    "from src.fcm import FCM\n",
    "from src.fcm import train_epoch, eval_epoch, acc, f1\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from src.dataset import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the normalization parameters\n",
    "means_std_path = os.path.join(ROOT_DIR, \"data\", \"MMHS150K\", \"means_stds.csv\")\n",
    "means_stds = pd.read_csv(means_std_path)\n",
    "\n",
    "# Minimal transformation for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[means_stds[\"mean_red\"][0], means_stds[\"mean_green\"][0], means_stds[\"mean_blue\"][0]],\n",
    "        std=[means_stds[\"std_red\"][0], means_stds[\"std_green\"][0], means_stds[\"std_blue\"][0]]\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file=os.path.join(ROOT_DIR, \"data\", \"MMHS150K\", \"MMHS150K_with_img_text.csv\"),\n",
    "    img_dir=os.path.join(ROOT_DIR, \"data\", \"MMHS150K\", \"img_resized/\"),\n",
    "    split=\"train\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "eval_dataset = CustomDataset(\n",
    "    csv_file=os.path.join(ROOT_DIR, \"data\", \"MMHS150K\", \"MMHS150K_with_img_text.csv\"),\n",
    "    img_dir=os.path.join(ROOT_DIR, \"data\", \"MMHS150K\", \"img_resized/\"),\n",
    "    split=\"val\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metrics_log(metrics_names, metrics_log, new_metrics_dict):\n",
    "    for i in range(len(metrics_names)):\n",
    "        curr_metric_name = metrics_names[i]\n",
    "        metrics_log[i].append(new_metrics_dict[curr_metric_name])\n",
    "    return metrics_log\n",
    "\n",
    "\n",
    "def train_cycle(model, optimizer, criterion, metrics, train_loader, test_loader, tokenizer, n_epochs, device):\n",
    "    train_loss_log,  test_loss_log = [], []\n",
    "    metrics_names = list(metrics.keys())\n",
    "    train_metrics_log = [[] for i in range(len(metrics))]\n",
    "    test_metrics_log = [[] for i in range(len(metrics))]\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {0} of {1}\".format(epoch, n_epochs))\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_loader, tokenizer, device)\n",
    "\n",
    "        test_loss, test_metrics = eval_epoch(model, criterion, metrics, test_loader, tokenizer, device)\n",
    "\n",
    "        train_loss_log.append(train_loss)\n",
    "        train_metrics_log = update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
    "\n",
    "        test_loss_log.append(test_loss)\n",
    "        test_metrics_log = update_metrics_log(metrics_names, test_metrics_log, test_metrics)\n",
    "\n",
    "    return train_metrics_log, test_metrics_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the tokenization function \n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# MAYBE LOOK FOR ONE MADE FOR TWEETS\n",
    "\n",
    "# Create the model\n",
    "vocab_size = len(tokenizer)\n",
    "fcm = FCM(device, vocab_size, batch_size, output_size=1, freeze_image_model=True, freeze_text_model=False).to(device)\n",
    "\n",
    "# Choose the optimizer and the loss function\n",
    "optimizer = torch.optim.Adam(fcm.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Choose the metrics\n",
    "metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "train_metrics_log, test_metrics_log = train_cycle(fcm, optimizer, criterion, metrics, train_loader, eval_loader, tokenizer, n_epochs, device)\n",
    "\n",
    "# Save the model\n",
    "model_weights_dir = os.path.join(ROOT_DIR, \"results\", \"model_weights\")\n",
    "if not os.path.exists(model_weights_dir):\n",
    "    os.makedirs(model_weights_dir)\n",
    "torch.save(fcm.state_dict(), os.path.join(model_weights_dir, \"fcm.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
