{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import os\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "import sys\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from src.dataset import CustomDataset\n",
    "from src.fcm import FCM\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from src.dataset import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the normalization parameters\n",
    "means_std_path = os.path.join(ROOT_DIR, \"data\", \"MMHS150K\", \"means_stds.csv\")\n",
    "means_stds = pd.read_csv(means_std_path)\n",
    "\n",
    "# Minimal transformation for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[means_stds[\"mean_red\"][0], means_stds[\"mean_green\"][0], means_stds[\"mean_blue\"][0]],\n",
    "        std=[means_stds[\"std_red\"][0], means_stds[\"std_green\"][0], means_stds[\"std_blue\"][0]]\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file=os.path.join(ROOT_DIR, \"data\", \"MMHS150K\", \"MMHS150K_with_img_text.csv\"),\n",
    "    img_dir=os.path.join(ROOT_DIR, \"data\", \"MMHS150K\", \"img_resized/\"),\n",
    "    split=\"train\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "eval_dataset = CustomDataset(\n",
    "    csv_file=os.path.join(ROOT_DIR, \"data\", \"MMHS150K\", \"MMHS150K_with_img_text.csv\"),\n",
    "    img_dir=os.path.join(ROOT_DIR, \"data\", \"MMHS150K\", \"img_resized/\"),\n",
    "    split=\"val\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_epoch(model, optimizer, criterion, metrics, train_loader, tokenizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_metrics = dict(zip(metrics.keys(), torch.zeros(len(metrics))))\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, data_dict in enumerate(train_loader):\n",
    "        # Get the input data\n",
    "        image = data_dict[\"image\"].to(device)\n",
    "        label = data_dict[\"label\"].to(device)\n",
    "        tweet_text = data_dict[\"tweet_text\"]\n",
    "        img_text = data_dict[\"img_text\"]\n",
    "                    \n",
    "        # Pass the text through the tokenizer and turn it into a tensor\n",
    "        tweet_text = tokenizer(tweet_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        img_text = tokenizer(img_text, padding=True, truncation=True, return_tensors=\"pt\")    \n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(image, tweet_text, img_text).squeeze(0)\n",
    "        output = torch.nn.Sigmoid()(output)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output, label.float().unsqueeze(1))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the metrics\n",
    "        with torch.no_grad():\n",
    "            predictions = output.argmax(dim=1)\n",
    "            epoch_loss += loss.item()\n",
    "            for name, metric in metrics.items():\n",
    "                epoch_metrics[name] += metric(predictions, label)\n",
    "                \n",
    "    epoch_loss /= len(train_loader)\n",
    "    for k in epoch_metrics.keys():\n",
    "        epoch_metrics[k] /= len(train_loader)\n",
    "    \n",
    "    return epoch_loss, epoch_metrics\n",
    "\n",
    "# Evaluate the model\n",
    "def eval_epoch(model, criterion, metrics, val_loader, tokenizer, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_metrics = dict(zip(metrics.keys(), torch.zeros(len(metrics))))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data_dict in enumerate(val_loader):\n",
    "            # Get the input data\n",
    "            image = data_dict[\"image\"].to(device)\n",
    "            label = data_dict[\"label\"].to(device)\n",
    "            tweet_text = data_dict[\"tweet_text\"]\n",
    "            img_text = data_dict[\"img_text\"]\n",
    "            \n",
    "            tweet_text = tokenizer(tweet_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            img_text = tokenizer(img_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(image, tweet_text, img_text)\n",
    "            \n",
    "            # Compute predictions\n",
    "            predictions = output.argmax(dim=1)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(output, label.float().unsqueeze(1))\n",
    "            \n",
    "            # Compute the metrics\n",
    "            epoch_loss += loss.item()\n",
    "            for name, metric in metrics.items():\n",
    "                epoch_metrics[name] += metric(predictions, label)\n",
    "                \n",
    "    epoch_loss /= len(val_loader)\n",
    "    for k in epoch_metrics.keys():\n",
    "        epoch_metrics[k] /= len(val_loader)\n",
    "        \n",
    "    return epoch_loss, epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(train_losses, val_losses, train_metrics, val_metrics):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    \n",
    "    axs[0, 0].plot(train_losses, label=\"train\")\n",
    "    axs[0, 0].plot(val_losses, label=\"val\")\n",
    "    axs[0, 0].set_title(\"Loss\")\n",
    "    axs[0, 0].legend()\n",
    "    \n",
    "    for i, (train_metric, val_metric) in enumerate(zip(train_metrics.keys(), val_metrics.keys())):\n",
    "        axs[1, i].plot(train_metrics[train_metric], label=\"train\")\n",
    "        axs[1, i].plot(val_metrics[val_metric], label=\"val\")\n",
    "        axs[1, i].set_title(train_metric)\n",
    "        axs[1, i].legend()\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "#actually you have to use validation for each step of training, but now we will focus only on the toy example and will track the perfromance on test\n",
    "def update_metrics_log(metrics_names, metrics_log, new_metrics_dict):\n",
    "    for i in range(len(metrics_names)):\n",
    "        curr_metric_name = metrics_names[i]\n",
    "        metrics_log[i].append(new_metrics_dict[curr_metric_name])\n",
    "    return metrics_log\n",
    "\n",
    "\n",
    "def train_cycle(model, optimizer, criterion, metrics, train_loader, test_loader, tokenizer, n_epochs, device):\n",
    "    train_loss_log,  test_loss_log = [], []\n",
    "    metrics_names = list(metrics.keys())\n",
    "    train_metrics_log = [[] for i in range(len(metrics))]\n",
    "    test_metrics_log = [[] for i in range(len(metrics))]\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {0} of {1}\".format(epoch, n_epochs))\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_loader, tokenizer, device)\n",
    "\n",
    "        test_loss, test_metrics = eval_epoch(model, criterion, metrics, test_loader, tokenizer, device)\n",
    "\n",
    "        train_loss_log.append(train_loss)\n",
    "        train_metrics_log = update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
    "\n",
    "        test_loss_log.append(test_loss)\n",
    "        test_metrics_log = update_metrics_log(metrics_names, test_metrics_log, test_metrics)\n",
    "\n",
    "        plot_training(train_loss_log, test_loss_log, metrics_names, train_metrics_log, test_metrics_log)\n",
    "    return train_metrics_log, test_metrics_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def f1(preds, target):\n",
    "    return f1_score(target, preds, average='macro')\n",
    "\n",
    "def acc(preds, target):\n",
    "    return accuracy_score(target, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the tokenization function \n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# MAYBE LOOK FOR ONE MADE FOR TWEETS\n",
    "\n",
    "# Create the model\n",
    "vocab_size = len(tokenizer)\n",
    "fcm = FCM(device, vocab_size, batch_size, output_size=1, freeze_image_model=True, freeze_text_model=False).to(device)\n",
    "\n",
    "# Choose the optimizer and the loss function\n",
    "optimizer = torch.optim.Adam(fcm.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Choose the metrics\n",
    "metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "train_metrics_log, test_metrics_log = train_cycle(fcm, optimizer, criterion, metrics, train_loader, eval_loader, tokenizer, n_epochs, device)\n",
    "\n",
    "# Save the model\n",
    "model_weights_dir = os.path.join(ROOT_DIR, \"results\", \"model_weights\")\n",
    "if not os.path.exists(model_weights_dir):\n",
    "    os.makedirs(model_weights_dir)\n",
    "torch.save(fcm.state_dict(), os.path.join(model_weights_dir, \"fcm.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
