{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "from src.utils import ROOT_DIR\n",
    "import sys\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from src.dataset import CustomDataset\n",
    "from src.fcm import FCM\n",
    "from src.fcm import train_epoch, eval_epoch, acc, f1\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from src.dataset import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "DATASET_PATH = os.path.join(ROOT_DIR, 'data', 'MMHS150K', 'MMHS150K_with_img_text.csv')\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "# Load the saved predictions\n",
    "\n",
    "PREDICTIONS_PATH = os.path.join(ROOT_DIR,'data', 'results','FCM', 'fcm_predictions.json')\n",
    "OUTPUTS_PATH = os.path.join(ROOT_DIR, 'data','results','FCM', 'fcm_outputs.json')\n",
    "\n",
    "import json\n",
    "with open(PREDICTIONS_PATH, 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "with open(OUTPUTS_PATH, 'r') as f:\n",
    "    outputs = json.load(f)\n",
    "    \n",
    "# For each index, get the binary_hate and label\n",
    "pred_keys = [int(k) for k in predictions.keys()]\n",
    "\n",
    "df_test = df[df[\"index\"].isin(pred_keys)]\n",
    "df_test[\"pred\"] = [predictions[str(k)] for k in df_test[\"index\"]]\n",
    "df_test[\"output\"] = [outputs[str(k)] for k in df_test[\"index\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare \"binary_hate\" and \"pred\" to get the accuracy and F1 score\n",
    "acc_score = acc(df_test[\"binary_hate\"], df_test[\"pred\"])\n",
    "f1_score = f1(df_test[\"binary_hate\"], df_test[\"pred\"])\n",
    "\n",
    "print(f\"Accuracy: {acc_score}\")\n",
    "print(f\"F1 score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "fpr, tpr, _ = roc_curve(df_test[\"binary_hate\"], df_test[\"output\"])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve FCM (area = %0.2f)' % roc_auc)\n",
    "# Add the random line\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', label='Random')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the outputs / predictions\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].hist(df_test[\"output\"], bins=50)\n",
    "ax[0].set_title(\"Distribution of the outputs\")\n",
    "ax[0].set_xlabel(\"Output\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "ax[1].hist(df_test[\"pred\"], bins=50)\n",
    "ax[1].set_title(\"Distribution of the predictions\")\n",
    "ax[1].set_xlabel(\"Prediction\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(df_test[\"binary_hate\"], df_test[\"pred\"])\n",
    "\n",
    "print(\"True positive rate: \", cm[1, 1] / (cm[1, 1] + cm[1, 0]))\n",
    "print(\"True negative rate: \", cm[0, 0] / (cm[0, 0] + cm[0, 1]))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_meme(df, img_dir, index):\n",
    "    row = df[df[\"index\"] == index]\n",
    "    text = row[\"tweet_text_clean\"].values[0]\n",
    "    label = row[\"binary_hate\"].values[0]\n",
    "    \n",
    "    image = Image.open(os.path.join(img_dir, str(index) + \".jpg\"))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {label}, Text: {text}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = os.path.join(ROOT_DIR, 'data', 'MMHS150K', \"img_resized\")\n",
    "\n",
    "# Using the outputs, get the top 10 most hateful tweets and the top 10 least hateful tweets\n",
    "df_test[\"output\"] = np.array(df_test[\"output\"])\n",
    "df_test[\"index\"] = np.array(df_test[\"index\"])\n",
    "\n",
    "top_10_hateful = df_test.sort_values(by=\"output\", ascending=False).head(10)\n",
    "top_10_least_hateful = df_test.sort_values(by=\"output\", ascending=True).head(10)\n",
    "\n",
    "for index in top_10_hateful[\"index\"]:\n",
    "    plot_meme(df_test, IMG_DIR, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in top_10_least_hateful[\"index\"]:\n",
    "    plot_meme(df_test, IMG_DIR, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
